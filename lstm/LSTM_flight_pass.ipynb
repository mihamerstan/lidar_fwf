{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from laspy.file import File\n",
    "from pickle import dump, load\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_line_gap_break = 7000 # threshold over which scan_gap indicates a new scan line\n",
    "min_pt_count = 1700 # in a scan line, otherwise line not used\n",
    "max_pt_count = 2000 # in a scan line, otherwise line not used\n",
    "seq_len = 25\n",
    "num_scan_lines = 24 # to use as training set\n",
    "val_split = 0.25\n",
    "\n",
    "# LSTM Model parameters\n",
    "num_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "input_size = 3 # x, y, z currently\n",
    "hidden_size = 9 # hidden features\n",
    "num_layers = 1 # Default is 1, 2 is a stacked LSTM\n",
    "output_dim = 3 # x,y,z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first_return_df has been processed in the following ways:  \n",
    "* Removed outliers outside of [0.01,0.99] percentile range\n",
    "* Normalized xyz values to [0,1]\n",
    "* Mapped each point to a scan line index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_return_df = pd.read_pickle(\"../Data/parking_lot/first_returns_modified_164239.pkl\")\n",
    "# sc = load(open('../Data/parking_lot/164239_scaler.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_return_df['abs_scan_angle_deg'] = abs(first_return_df['scan_angle_deg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: x_scaled, y_scaled, and z_scaled MUST be the first 3 features\n",
    "feature_list = [\n",
    "    'x_scaled',\n",
    "    'y_scaled',\n",
    "    'z_scaled',\n",
    "    'scan_line_idx',\n",
    "    'scan_angle_deg',\n",
    "    'abs_scan_angle_deg'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MinMax the xyz coordinates\n",
    "sc = MinMaxScaler()\n",
    "# df2 = pd.DataFrame(sc.fit_transform(first_return_df[['x_scaled','y_scaled','z_scaled']]),columns=['x_norm','y_norm','z_norm'],\\\n",
    "#                    index=first_return_df.index)\n",
    "# first_return_df = pd.concat([first_return_df,df2],axis=1)\n",
    "sc.fit_transform(first_return_df[feature_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Extract tensor of scan lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of points per scan line\n",
    "scan_line_pt_count = first_return_df.groupby('scan_line_idx').count()['gps_time']\n",
    "\n",
    "# Identify the indices for points at end of scan lines\n",
    "scan_break_idx = first_return_df[(first_return_df['scan_gap']>scan_line_gap_break)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tensor\n",
    "line_count = ((scan_line_pt_count>min_pt_count)&(scan_line_pt_count<max_pt_count)).sum()\n",
    "scan_line_tensor = torch.randn([line_count,min_pt_count,len(feature_list)])\n",
    "\n",
    "# Collect the scan lines longer than min_pt_count\n",
    "# For each, collect the first min_pt_count points\n",
    "i=0\n",
    "for line,count in enumerate(scan_line_pt_count):\n",
    "    if (count>min_pt_count)&(count<max_pt_count):\n",
    "        try:\n",
    "            line_idx = scan_break_idx[line-1]\n",
    "            scan_line_tensor[i,:,:] = torch.Tensor(first_return_df.iloc\\\n",
    "                                      [line_idx:line_idx+min_pt_count][feature_list].values)\n",
    "            i+=1\n",
    "        except RuntimeError:\n",
    "            print(\"line: \",line)\n",
    "            print(\"line_idx: \",line_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Generate the data\n",
    "First, naively: Concatenate all the sequences from all scan lines  \n",
    "To Do: Track scan line, scan angle, decide where to incorporate these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_windows(data, seq_length, line_num, x, y):\n",
    "    for i in range(len(data)-seq_length):\n",
    "        # Index considers previous lines\n",
    "        idx = i+line_num*(min_pt_count-seq_length)\n",
    "        _x = data[i:(i+seq_length)]\n",
    "        _y = data[i+seq_length,:3] # Assumes xyz are the first 3 features in scan_line_tensor\n",
    "        x[idx,:,:] = _x\n",
    "        y[idx,:,:] = _y\n",
    "\n",
    "    return x,y\n",
    "\n",
    "def generate_samples(data,min_pt_count,seq_len,num_scan_lines,val_split,starting_line=1000):\n",
    "    '''\n",
    "    Function generates training and validation samples for predicting the next point in the sequence.\n",
    "    Inputs:\n",
    "        data: 3-Tensor with dimensions: i) the number of viable scan lines in the flight pass, \n",
    "                                        ii) the minimum number of points in the scan line,\n",
    "                                        iii) 3 (xyz, or feature count)\n",
    "    \n",
    "    '''\n",
    "    # Create generic x and y tensors\n",
    "    x = torch.ones([(min_pt_count-seq_len)*num_scan_lines,seq_len,len(feature_list)]) \n",
    "    y = torch.ones([(min_pt_count-seq_len)*num_scan_lines,1,3])\n",
    "    i=0\n",
    "    # Cycle through the number of scan lines requested, starting somewhere in the middle\n",
    "    for line_idx in range(starting_line,starting_line+num_scan_lines):\n",
    "        x,y = sliding_windows(data[line_idx,:,:],seq_len,line_idx-starting_line, x, y)\n",
    "#     x_train,y_train,x_val,y_val = train_val_split(x,y,val_split)\n",
    "    return x,y\n",
    "#     return x_train,y_train,x_val,y_val\n",
    "\n",
    "def train_val_split(x,y,val_split):   \n",
    "    # Training/Validation split\n",
    "    # For now, we'll do the last part of the dataset as validation...shouldn't matter?\n",
    "    train_val_split_idx = int(x.shape[0]*(1-val_split))\n",
    "    x_train = x[:train_val_split_idx,:,:]\n",
    "    x_val = x[train_val_split_idx:,:,:]\n",
    "    y_train = y[:train_val_split_idx,:,:]\n",
    "    y_val = y[train_val_split_idx:,:,:]\n",
    "    \n",
    "    return x_train,y_train,x_val,y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train,y_train,x_val,y_val = generate_samples(scan_line_tensor,min_pt_count,seq_len,num_scan_lines,val_split)\n",
    "x,y = generate_samples(scan_line_tensor,min_pt_count,seq_len,num_scan_lines,val_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 2])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = torch.Tensor([[[1,2],[4,5],[7,8]],[[-1,-1,],[-2,-2,],[-3,-3,]]])\n",
    "z.view(-1,3)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Train the model  \n",
    "Borrowing a lot of code from here: https://github.com/spdin/time-series-prediction-lstm-pytorch/blob/master/Time_Series_Prediction_with_LSTM_Using_PyTorch.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, output_dim, input_size, hidden_size, num_layers, seq_len):\n",
    "        super(LSTM, self).__init__()\n",
    "        # output_dim = 3: X,Y,Z\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # inputs_size = 3: X,Y,Z (could be larger in the future if we add features here)\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # Not sure what to do here, larger than input size?\n",
    "        self.hidden_size = hidden_size\n",
    "        # Passes from above\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size))\n",
    "        \n",
    "        c_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size))\n",
    "        \n",
    "        # Propagate input through LSTM\n",
    "        ula, (h_out, _) = self.lstm(x, (h_0, c_0))\n",
    "        \n",
    "        # In case multiple LSTM layers are used, this predicts using only the last layer\n",
    "        h_out = h_out.view(num_layers,-1, self.hidden_size)\n",
    "        out = self.fc(h_out[-1,:,:])\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM(output_dim, input_size, hidden_size, num_layers, seq_len)\n",
    "\n",
    "criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = lstm(x)\n",
    "    optimizer.zero_grad()\n",
    "    # obtain the loss function\n",
    "    loss = criterion(outputs.unsqueeze(1), y)\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    if epoch % 20 == 0:\n",
    "      print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(lstm,x_train,y_train,x_val,y_val):\n",
    "    # Training loss\n",
    "    y_train_pred = lstm(x_train).detach()\n",
    "    train_loss = criterion(y_train_pred.unsqueeze(1), y_train)\n",
    "    # Validation loss\n",
    "    y_val_pred = lstm(x_val).detach()\n",
    "    val_loss = criterion(y_val_pred.unsqueeze(1), y_val)\n",
    "    print(\"Training Loss: {:.3e}\\nValidation Loss: {:.3e}\".format(train_loss,val_loss))\n",
    "\n",
    "calculate_loss(lstm,x_train,y_train,x_val,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(sample_num):\n",
    "    in_seq = sc.inverse_transform(x[sample_num])\n",
    "    pred_point = sc.inverse_transform(lstm(x[sample_num].unsqueeze(0)).detach())\n",
    "    true_point = sc.inverse_transform(y[sample_num])\n",
    "    plt.plot(in_seq[:,0],in_seq[:,2],'x')\n",
    "    plt.plot(pred_point[0,0],pred_point[0,2],'ro')\n",
    "    plt.plot(true_point[0,0],true_point[0,2],'go')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(22000,22010):\n",
    "    print_results(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.inverse_transform(y[22])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adj GPS Time: Set both timestamps to zero for the first record\n",
    "def adjust_time(df,time_field):\n",
    "    # Function adds adj_gps_time to points or pulse dataframe, set to zero at the minimum timestamp.\n",
    "    df['adj_gps_time'] = df[time_field] - df[time_field].min()\n",
    "    return df\n",
    "\n",
    "def label_returns(las_df):\n",
    "    '''\n",
    "    Parses the flag_byte into number of returns and return number, adds these fields to las_df.\n",
    "    Input - las_df - dataframe from .laz or .lz file\n",
    "    Output - first_return_df - only the first return points from las_df.\n",
    "           - las_df - input dataframe with num_returns and return_num fields added \n",
    "    '''\n",
    "    \n",
    "    las_df['num_returns'] = np.floor(las_df['flag_byte']/16).astype(int)\n",
    "    las_df['return_num'] = las_df['flag_byte']%16\n",
    "    first_return_df = las_df[las_df['return_num']==1]\n",
    "    first_return_df = first_return_df.reset_index(drop=True)\n",
    "    return first_return_df, las_df\n",
    "\n",
    "\n",
    "def pull_first_scan_gap(df):\n",
    "    # Separate return num, only keep the first returns, add scan_gap, sort\n",
    "    df['num_returns'] = np.floor(df['flag_byte']/16).astype(int)\n",
    "    df['return_num'] = df['flag_byte']%16\n",
    "    \n",
    "    first_return_wall = df[df['return_num']==1]\n",
    "    \n",
    "    # Outliers\n",
    "    # Remove outliers outside of [.01,.99] percentiles\n",
    "    a = first_return_wall[['x_scaled','y_scaled','z_scaled']].quantile([.01,.99])\n",
    "    first_return_wall = first_return_wall[(first_return_wall['x_scaled']>a.iloc[0]['x_scaled'])&\\\n",
    "                                         (first_return_wall['x_scaled']<a.iloc[1]['x_scaled'])&\\\n",
    "                                         (first_return_wall['y_scaled']>a.iloc[0]['y_scaled'])&\\\n",
    "                                         (first_return_wall['y_scaled']<a.iloc[1]['y_scaled'])&\\\n",
    "                                         (first_return_wall['z_scaled']>a.iloc[0]['z_scaled'])&\\\n",
    "                                         (first_return_wall['z_scaled']<a.iloc[1]['z_scaled'])]\n",
    "    \n",
    "    first_return_wall.sort_values(by=['gps_time'],inplace=True)\n",
    "    first_return_wall.reset_index(inplace=True)\n",
    "    first_return_wall.loc[1:,'scan_gap'] = [first_return_wall.loc[i+1,'scan_angle'] - first_return_wall.loc[i,'scan_angle'] for i in range(first_return_wall.shape[0]-1)]\n",
    "    first_return_wall.loc[0,'scan_gap'] = 0\n",
    "    first_return_wall['scan_angle_deg'] = first_return_wall['scan_angle']*.006\n",
    "    return first_return_wall\n",
    "\n",
    "# Load LAS points\n",
    "las_df = pd.read_hdf(\"../Data/parking_lot/las_points_164239.lz\")\n",
    "# Separate out the first returns only\n",
    "las_df = adjust_time(las_df,'gps_time')\n",
    "# Sort records by timestamp\n",
    "las_df.sort_values(by=['adj_gps_time'],inplace=True)\n",
    "# TO DO: consider only last returns?\n",
    "# First returns only\n",
    "first_return_df = pull_first_scan_gap(las_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Save the scaler\n",
    "dump(lstm, open('models/lstm_unidirectional_model_1000epochs_25scan_lines_8_25_20.pkl','wb'))\n",
    "# dump(sc, open('models/SCALER.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Identify the indices for points at end of scan lines\n",
    "scan_break_idx = first_return_df[(first_return_df['scan_gap']>scan_line_gap_break)].index\n",
    "\n",
    "# # Concat adds index 0 as 0th scan line\n",
    "_right = pd.DataFrame(data=range(1,len(scan_break_idx)+1),index=scan_break_idx,columns=['scan_line_idx'])\n",
    "right = pd.concat([pd.DataFrame(data=[0],index=[0],columns=['scan_line_idx']),_right])\n",
    "first_return_df = pd.merge_asof(first_return_df,right,left_index=True,right_index=True,direction='backward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
