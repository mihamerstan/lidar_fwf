{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from laspy.file import File\n",
    "from pickle import dump, load\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as udata\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data parameters\n",
    "scan_line_gap_break = 7000 # threshold over which scan_gap indicates a new scan line\n",
    "min_pt_count = 1700 # in a scan line, otherwise line not used\n",
    "max_pt_count = 2000 # in a scan line, otherwise line not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpu or cpu\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first_return_df has been processed in the following ways:  \n",
    "* Removed outliers outside of [0.01,0.99] percentile range\n",
    "* Normalized xyz values to [0,1]\n",
    "* Mapped each point to a scan line index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_return_df = pd.read_pickle(\"../../Data/parking_lot/first_returns_modified_164239.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Extract tensor of scan lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of points per scan line\n",
    "scan_line_pt_count = first_return_df.groupby('scan_line_idx').count()['gps_time']\n",
    "\n",
    "# Identify the indices for points at end of scan lines\n",
    "scan_break_idx = first_return_df[(first_return_df['scan_gap']>scan_line_gap_break)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "scan_line_tensor is a 3-D Tensor: [num_scan_lines,num_sequences,4]\n",
    "'''\n",
    "line_count = ((scan_line_pt_count>min_pt_count)&(scan_line_pt_count<max_pt_count)).sum()\n",
    "scan_line_tensor = torch.randn([line_count,min_pt_count,4])\n",
    "\n",
    "# Collect the scan lines longer than min_pt_count\n",
    "# For each, collect the first min_pt_count points\n",
    "i=0\n",
    "for line,count in enumerate(scan_line_pt_count):\n",
    "    if (count>min_pt_count)&(count<max_pt_count):\n",
    "        try:\n",
    "            line_idx = scan_break_idx[line-1]\n",
    "            scan_line_tensor[i,:,:3] = torch.Tensor(first_return_df.iloc\\\n",
    "                                      [line_idx:line_idx+min_pt_count][['x_scaled','y_scaled','z_scaled']].values)\n",
    "            scan_line_tensor[i,:,3] = line\n",
    "            i+=1\n",
    "        except RuntimeError:\n",
    "            print(\"line: \",line)\n",
    "            print(\"line_idx: \",line_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Do:  \n",
    "- Align the scan angle of the first points in each sequence\n",
    "- Identify missing points\n",
    "- Fill missing points with 0,0,0?\n",
    "- Look for skipped flight passes (using line_idx)\n",
    "- Create training images from the larger grid\n",
    "- Create masks for missing points\n",
    "- Blank out masked points for training data\n",
    "- Build model to accept 3-channel image + mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_tensor(tensor):\n",
    "    # Function takes a 3-D tensor, performs minmax scaling to [0,1] along the third dimension.\n",
    "    # First 2 dimensions are flattened\n",
    "    a,b,c = tensor.shape\n",
    "    # Flatten first two dimensions\n",
    "    flat_tensor = tensor.view(-1,c)\n",
    "    sc =  MinMaxScaler()\n",
    "    flat_norm_tensor = sc.fit_transform(flat_tensor)\n",
    "    # Reshape to original\n",
    "    output = flat_norm_tensor.reshape([a,b,c])\n",
    "    return torch.Tensor(output), sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_line_tensor_norm, sc = min_max_tensor(scan_line_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Generate the data\n",
    "First, naively: Concatenate all the sequences from all scan lines  \n",
    "To Do: Track scan line, scan angle, decide where to incorporate these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_windows(data, seq_length, line_num, x, y):\n",
    "    for i in range(len(data)-seq_length):\n",
    "        # Index considers previous lines\n",
    "        idx = i+line_num*(min_pt_count-seq_length)\n",
    "        _x = data[i:(i+seq_length)]\n",
    "        _y = data[i+seq_length,:3] # Assumes xyz are the first 3 features in scan_line_tensor\n",
    "        x[idx,:,:] = _x\n",
    "        y[idx,:,:] = _y\n",
    "\n",
    "    return x,y\n",
    "\n",
    "def generate_samples(data,min_pt_count,seq_len,num_scan_lines,val_split,starting_line=1000):\n",
    "    '''\n",
    "    Function generates training and validation samples for predicting the next point in the sequence.\n",
    "    Inputs:\n",
    "        data: 3-Tensor with dimensions: i) the number of viable scan lines in the flight pass, \n",
    "                                        ii) the minimum number of points in the scan line,\n",
    "                                        iii) 3 (xyz, or feature count)\n",
    "    \n",
    "    '''\n",
    "    # Create generic x and y tensors\n",
    "    x = torch.ones([(min_pt_count-seq_len)*num_scan_lines,seq_len,len(feature_list)]) \n",
    "    y = torch.ones([(min_pt_count-seq_len)*num_scan_lines,1,3])\n",
    "    i=0\n",
    "    # Cycle through the number of scan lines requested, starting somewhere in the middle\n",
    "    for line_idx in range(starting_line,starting_line+num_scan_lines):\n",
    "        x,y = sliding_windows(data[line_idx,:,:],seq_len,line_idx-starting_line, x, y)\n",
    "    x_train,y_train,x_val,y_val = train_val_split(x,y,val_split)\n",
    "    return x_train,y_train,x_val,y_val\n",
    "\n",
    "def train_val_split(x,y,val_split):   \n",
    "    # Training/Validation split\n",
    "    # For now, we'll do the last part of the dataset as validation...shouldn't matter?\n",
    "    train_val_split_idx = int(x.shape[0]*(1-val_split))\n",
    "    x_train = x[:train_val_split_idx,:,:]\n",
    "    x_val = x[train_val_split_idx:,:,:]\n",
    "    y_train = y[:train_val_split_idx,:,:]\n",
    "    y_val = y[train_val_split_idx:,:,:]\n",
    "    \n",
    "    return x_train,y_train,x_val,y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train,x_val,y_val = generate_samples(scan_line_tensor_norm,min_pt_count,seq_len,num_scan_lines,val_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Train the model  \n",
    "Borrowing a lot of code from here: https://github.com/spdin/time-series-prediction-lstm-pytorch/blob/master/Time_Series_Prediction_with_LSTM_Using_PyTorch.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, output_dim, input_size, hidden_size, num_layers, seq_len):\n",
    "        super(LSTM, self).__init__()\n",
    "        # output_dim = 3: X,Y,Z\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # inputs_size = 3: X,Y,Z (could be larger in the future if we add features here)\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # Not sure what to do here, larger than input size?\n",
    "        self.hidden_size = hidden_size\n",
    "        # Passes from above\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size)).to(device)\n",
    "        \n",
    "        c_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size)).to(device)\n",
    "        \n",
    "        # Propagate input through LSTM\n",
    "        ula, (h_out, _) = self.lstm(x, (h_0, c_0))\n",
    "        \n",
    "        # In case multiple LSTM layers are used, this predicts using only the last layer\n",
    "        h_out = h_out.view(num_layers,-1, self.hidden_size)\n",
    "        out = self.fc(h_out[-1,:,:])\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function that weights the loss according to coordinate ranges (xmax-xmin, ymax-ymin, zmax-zmin)\n",
    "def weighted_MSELoss(pred,true,sc):\n",
    "    '''Assumes that x,y,z are the first 3 features in sc scaler'''\n",
    "    ranges = torch.Tensor(sc.data_max_[:3]-sc.data_min_[:3])\n",
    "    raw_loss = torch.zeros(3,dtype=float)\n",
    "    crit = torch.nn.MSELoss()\n",
    "    for i in range(3):\n",
    "        raw_loss[i] = crit(pred[:,:,i], true[:,:,i])\n",
    "    return (ranges * raw_loss).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(lstm,x_train,y_train,x_val,y_val):\n",
    "    # Training loss\n",
    "    y_train_pred = lstm(x_train).detach()\n",
    "    train_loss = weighted_MSELoss(y_train_pred.unsqueeze(1), y_train,sc)\n",
    "    # Validation loss\n",
    "    y_val_pred = lstm(x_val).detach()\n",
    "    val_loss = weighted_MSELoss(y_val_pred.unsqueeze(1), y_val,sc)\n",
    "    print(\"Training Loss: {:.4f}\\nValidation Loss: {:.4f}\".format(train_loss,val_loss))\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LidarLstmDataset(udata.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super(LidarLstmDataset, self).__init__()\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.x[index],self.y[index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tl,vl = [],[]\n",
    "lstm_local = LSTM(output_dim, len(feature_list), hidden_size, num_layers, seq_len)\n",
    "lstm = lstm_local.to(device)\n",
    "\n",
    "# Create the dataloader\n",
    "train_dataset = LidarLstmDataset(x_train,y_train)\n",
    "val_dataset = LidarLstmDataset(x_val,y_val)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1024, num_workers=1, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, num_workers=1, shuffle=False)\n",
    "\n",
    "# criterion = torch.nn.MSELoss()    # mean-squared error for regression\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for x,y in train_loader:\n",
    "        outputs = lstm(x.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        # obtain the loss function\n",
    "        loss = weighted_MSELoss(outputs.unsqueeze(1), y.to(device),sc)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "    if epoch % 20 == 0:\n",
    "        print(\"*\"*30)\n",
    "        print(\"Epoch: %d, loss: %1.5f\\n\" % (epoch, loss.item()))\n",
    "#         tr, val = calculate_loss(lstm,x_train,y_train,x_val,y_val)\n",
    "#         tl.append(tr)\n",
    "#         vl.append(val)\n",
    "\n",
    "# Print loss plot\n",
    "# plt.plot(20*np.arange(len(tl)-1),tl[1:])\n",
    "# plt.plot(20*np.arange(len(vl)-1),vl[1:],'+')\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Weighted MSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print loss plot\n",
    "plt.plot(20*np.arange(len(tl)-10),tl[10:],label='Training')\n",
    "plt.plot(20*np.arange(len(vl)-10),vl[10:],'+',label='Validation')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Weighted MSE\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(x,y,lstm,sample_num):\n",
    "    fontsize=12\n",
    "    in_seq = sc.inverse_transform(x[sample_num])\n",
    "    pred_norm = (lstm(x[sample_num].unsqueeze(0).to(device)).view(-1,3).detach())\n",
    "    pred_point =     pred_norm.to('cpu')*(sc.data_max_[:3]-sc.data_min_[:3])+sc.data_min_[:3]\n",
    "    true_point = y[sample_num]*(sc.data_max_[:3]-sc.data_min_[:3])+sc.data_min_[:3]\n",
    "    plt.figure(figsize=[20,4])\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(in_seq[:,0],in_seq[:,1],'x')\n",
    "    plt.plot(pred_point[0,0],pred_point[0,1],'ro')\n",
    "    plt.plot(true_point[0,0],true_point[0,1],'go')\n",
    "    plt.xlabel(\"X\",fontsize=fontsize)\n",
    "    plt.ylabel(\"Y\",fontsize=fontsize)\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(in_seq[:,0],in_seq[:,2],'x')\n",
    "    plt.plot(pred_point[0,0],pred_point[0,2],'ro')\n",
    "    plt.plot(true_point[0,0],true_point[0,2],'go')\n",
    "    plt.xlabel(\"X\",fontsize=fontsize)\n",
    "    plt.ylabel(\"Z\",fontsize=fontsize)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(4120,4125):\n",
    "    print_results(x_train,y_train,lstm,i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "dir_name = '8_28_20_run2/'\n",
    "run_descriptor = 'seq_len_100'\n",
    "os.mkdir('models/'+dir_name)\n",
    "class Args(object):\n",
    "    def __init__(self):\n",
    "        self.scan_line_gap_break = scan_line_gap_break\n",
    "        self.min_pt_count = min_pt_count\n",
    "        self.max_pt_count = max_pt_count\n",
    "        self.seq_len = seq_len\n",
    "        self.num_scan_lines = num_scan_lines\n",
    "        self.val_split = val_split\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "        self.num_epochs = num_epochs\n",
    "        self.learning_rate = learning_rate\n",
    "args=Args()\n",
    "\n",
    "# Save the scaler\n",
    "dump(lstm, open('models/'+dir_name+run_descriptor+'.pkl','wb'))\n",
    "dump(sc, open('models/'+dir_name+'SCALER_'+run_descriptor+'.pkl', 'wb'))\n",
    "dump(args, open('models/'+dir_name+'args_'+run_descriptor+'.pkl', 'wb'))\n",
    "with open('models/'+dir_name+'args_'+run_descriptor+'.json', 'w') as json_file:\n",
    "    json.dump(json.dumps(args.__dict__), json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create .pts file of predictions\n",
    "Include the actual and the predicted, indicated with a binary flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pts_file(y_val,x_val,lstm,sc):\n",
    "    y_val_reinflate = np.concatenate((y_val[:,0,:]*(sc.data_max_[:3]-sc.data_min_[:3]) \\\n",
    "                                      +sc.data_min_[:3],np.zeros((y_val.shape[0],1))),axis=1)\n",
    "    out_df = pd.DataFrame(np.array(y_val_reinflate[:,:]),columns=['x','y','z','class'])\n",
    "    pred_norm = (lstm(x_val).view(-1,3).detach())\n",
    "    pred_reinflate = pred_norm*(sc.data_max_[:3]-sc.data_min_[:3])+sc.data_min_[:3]\n",
    "    pred_arr = np.concatenate((pred_reinflate,np.ones((pred_reinflate.shape[0],1))),axis=1)\n",
    "    out_df = out_df.append(pd.DataFrame(pred_arr,columns = out_df.columns)).reset_index(drop=True)\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df = create_pts_file(y_val,x_val,lstm,sc)\n",
    "out_df.to_csv(\"output_test.pts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep\n",
    "Already done, but this removes outliers and adds scan_line_idx to the first_return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adj GPS Time: Set both timestamps to zero for the first record\n",
    "def adjust_time(df,time_field):\n",
    "    # Function adds adj_gps_time to points or pulse dataframe, set to zero at the minimum timestamp.\n",
    "    df['adj_gps_time'] = df[time_field] - df[time_field].min()\n",
    "    return df\n",
    "\n",
    "def label_returns(las_df):\n",
    "    '''\n",
    "    Parses the flag_byte into number of returns and return number, adds these fields to las_df.\n",
    "    Input - las_df - dataframe from .laz or .lz file\n",
    "    Output - first_return_df - only the first return points from las_df.\n",
    "           - las_df - input dataframe with num_returns and return_num fields added \n",
    "    '''\n",
    "    \n",
    "    las_df['num_returns'] = np.floor(las_df['flag_byte']/16).astype(int)\n",
    "    las_df['return_num'] = las_df['flag_byte']%16\n",
    "    first_return_df = las_df[las_df['return_num']==1]\n",
    "    first_return_df = first_return_df.reset_index(drop=True)\n",
    "    return first_return_df, las_df\n",
    "\n",
    "\n",
    "def pull_first_scan_gap(df):\n",
    "    # Separate return num, only keep the first returns, add scan_gap, sort\n",
    "    df['num_returns'] = np.floor(df['flag_byte']/16).astype(int)\n",
    "    df['return_num'] = df['flag_byte']%16\n",
    "    \n",
    "    first_return_wall = df[df['return_num']==1]\n",
    "    \n",
    "    # Outliers\n",
    "    # Remove outliers outside of [.01,.99] percentiles\n",
    "    a = first_return_wall[['x_scaled','y_scaled','z_scaled']].quantile([.01,.99])\n",
    "    first_return_wall = first_return_wall[(first_return_wall['x_scaled']>a.iloc[0]['x_scaled'])&\\\n",
    "                                         (first_return_wall['x_scaled']<a.iloc[1]['x_scaled'])&\\\n",
    "                                         (first_return_wall['y_scaled']>a.iloc[0]['y_scaled'])&\\\n",
    "                                         (first_return_wall['y_scaled']<a.iloc[1]['y_scaled'])&\\\n",
    "                                         (first_return_wall['z_scaled']>a.iloc[0]['z_scaled'])&\\\n",
    "                                         (first_return_wall['z_scaled']<a.iloc[1]['z_scaled'])]\n",
    "    \n",
    "    first_return_wall.sort_values(by=['gps_time'],inplace=True)\n",
    "    first_return_wall.reset_index(inplace=True)\n",
    "    first_return_wall.loc[1:,'scan_gap'] = [first_return_wall.loc[i+1,'scan_angle'] - first_return_wall.loc[i,'scan_angle'] for i in range(first_return_wall.shape[0]-1)]\n",
    "    first_return_wall.loc[0,'scan_gap'] = 0\n",
    "    first_return_wall['scan_angle_deg'] = first_return_wall['scan_angle']*.006\n",
    "    return first_return_wall\n",
    "\n",
    "# Load LAS points\n",
    "las_df = pd.read_hdf(\"../../Data/parking_lot/las_points_164239.lz\")\n",
    "# Separate out the first returns only\n",
    "las_df = adjust_time(las_df,'gps_time')\n",
    "# Sort records by timestamp\n",
    "las_df.sort_values(by=['adj_gps_time'],inplace=True)\n",
    "# TO DO: consider only last returns?\n",
    "# First returns only\n",
    "first_return_df = pull_first_scan_gap(las_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Identify the indices for points at end of scan lines\n",
    "scan_break_idx = first_return_df[(first_return_df['scan_gap']>scan_line_gap_break)].index\n",
    "\n",
    "# # Concat adds index 0 as 0th scan line\n",
    "_right = pd.DataFrame(data=range(1,len(scan_break_idx)+1),index=scan_break_idx,columns=['scan_line_idx'])\n",
    "right = pd.concat([pd.DataFrame(data=[0],index=[0],columns=['scan_line_idx']),_right])\n",
    "first_return_df = pd.merge_asof(first_return_df,right,left_index=True,right_index=True,direction='backward')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
