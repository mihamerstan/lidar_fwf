{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling and Analysis - Sunset Park\n",
    "This notebook performs the central sampling and analysis of the paper. For each sample surface (defined by a set of xyz coordinates on the plane), the notebook generates the desired number of sample squares, collects the points in that square, generates the desired statistics, and aggregates over all the sample surfaces.  Most of the functions utilized are from the point_density_functions.py file.  \n",
    "\n",
    "Additional analyses:\n",
    "* Density at different wall heights\n",
    "* Missing points by scan angle, missing points on vertical vs. horizontal surfaces\n",
    "* Figure 12 analysis - mean orthogonal offset by flight pass, which generates the cross-pass error.\n",
    "\n",
    "Note:\n",
    "This notebook refers to Sunset Park 2019 dataset as 'laefer', NYCDOITT 2017 dataset as 'nyc' and the USGS 2014 dataset as 'usgs'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from laspy.file import File\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0,'..') # So we can import point_density_functions from parent directory\n",
    "from point_density_functions import *\n",
    "%load_ext autoreload\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laefer\n",
    "file_dir = '../../Data/parking_lot/'\n",
    "filenames =list(pd.read_csv(file_dir+\"filenames.txt\",header=None)[0])\n",
    "pt_files = list(pd.read_csv(file_dir+\"pt_files.txt\",header=None)[0])\n",
    "# NYC\n",
    "nyc_file_dir = '../../Data/NYC_topo/'\n",
    "nyc_pt_file = ['las_points_NYC_flightid_975172.lz']\n",
    "# USGS\n",
    "usgs_file_dir = '../../Data/USGS/'\n",
    "usgs_pt_files = ['las_points_flight_id_18TWK820985.lz']\n",
    "\n",
    "\n",
    "# Corresponds to LAS 1.2 Point Data Record Format 1\n",
    "columns_dublin_pt_cloud = [\n",
    "    'X',\n",
    "    'Y',\n",
    "    'Z',\n",
    "    'intensity',\n",
    "    'return_number_byte',\n",
    "    'classification_byte',\n",
    "    'scan_angle',\n",
    "    'user_data',\n",
    "    'pt_src_id',\n",
    "    'gps_time']\n",
    "\n",
    "columns_point_cloud = [\n",
    "    'X','Y','Z',\n",
    "    'intensity',\n",
    "    'flag_byte',\n",
    "    'classification_flags',\n",
    "    'classification_byte',\n",
    "    'user_data',\n",
    "    'scan_angle',\n",
    "    'pt_src_id',\n",
    "    'gps_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This works\n",
    "# for filename in filenames:\n",
    "#     create_df_hd5(file_dir,filename,columns_point_cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supporting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_out(SampleFlightList,pt_density_laefer_list,pt_density_nyc_list,pt_density_usgs_list,feet_from_point):\n",
    "    '''\n",
    "    Primary print function for point density, number of flight paths, point distance from plane, etc.\n",
    "    Function is tailored to the three Sunset Park scans discussed in the paper.\n",
    "    '''\n",
    "    # Laefer\n",
    "    sd_laefer_total = np.mean([ss.flight_list_laefer[0].sd_dist for ss in SampleFlightList])\n",
    "    sd_laefer_sample = np.mean([ss.flight_list_laefer[1].sd_dist for ss in SampleFlightList])\n",
    "    avg_flight_paths_laefer = np.mean([len(flight.flight_list_laefer) for flight in SampleFlightList])-2\n",
    "\n",
    "    phis_laefer_total = [ss.phi_laefer_total for ss in SampleFlightList]\n",
    "    phis_laefer_sample = [ss.phi_laefer_sample for ss in SampleFlightList]\n",
    "\n",
    "    print(\"2019 scan (Horizontal, {} samples): \\n\".format(len(SampleFlightList))+\"*\"*30)\n",
    "    print(\"Avg points per square: {:2.2f} points\".format((4 * feet_from_point**2)*np.mean(pt_density_laefer_list)))\n",
    "    print(\"Avg density: {:2.4f} pts/sqft (SD: {:2.4f})\".format(np.mean(pt_density_laefer_list),np.std(pt_density_laefer_list)))\n",
    "    print(\"Avg number of flight paths per square: {:2.2f}\".format(avg_flight_paths_laefer))\n",
    "    print(\"\\nphi_total: {:2.4f} (SD: {:2.4f})\".format(np.mean(phis_laefer_total),np.std(phis_laefer_total)))\n",
    "    print(\"phi_sample: {:2.4f} (SD: {:2.4f})\".format(np.mean(phis_laefer_sample),np.std(phis_laefer_sample)))\n",
    "    print(\"Total point dist from plane, SD: {:2.4f} feet\".format(sd_laefer_total))\n",
    "    print(\"Avg flight point dist from plane, SD: {:2.4f} feet\".format(sd_laefer_total/np.mean(phis_laefer_total)))\n",
    "\n",
    "    # NYC\n",
    "    phis_nyc_total = [ss.phi_nyc_total for ss in SampleFlightList]\n",
    "    phis_nyc_sample = [ss.phi_nyc_sample for ss in SampleFlightList]\n",
    "    avg_flight_paths_nyc = np.mean([len(flight.flight_list_nyc) for flight in SampleFlightList])-2\n",
    "\n",
    "    sd_nyc_total = np.mean([ss.flight_list_nyc[0].sd_dist for ss in SampleFlightList])\n",
    "    sd_nyc_sample = np.mean([ss.flight_list_nyc[1].sd_dist for ss in SampleFlightList])\n",
    "\n",
    "    print(\"\\n2017 scan (Horizontal, {} samples): \\n\".format(len(SampleFlightList))+\"*\"*30)\n",
    "    print(\"Avg points per square: {:2.2f} points\".format((4 * feet_from_point**2)*np.mean(pt_density_nyc_list)))\n",
    "    print(\"Avg density: {:2.4f} pts/sqft (SD: {:2.4f})\".format(np.mean(pt_density_nyc_list),np.std(pt_density_nyc_list)))\n",
    "    print(\"Avg number of flight paths per square: {:2.2f}\".format(avg_flight_paths_nyc))\n",
    "    print(\"\\nphi_total: {:2.4f} (SD: {:2.4f})\".format(np.mean(phis_nyc_total),np.std(phis_nyc_total)))\n",
    "    print(\"phi_sample: {:2.4f} (SD: {:2.4f})\".format(np.mean(phis_nyc_sample),np.std(phis_nyc_sample)))\n",
    "    print(\"Total point dist from plane, SD: {:2.4f} feet\".format(sd_nyc_total))\n",
    "    print(\"Avg flight point dist from plane, SD: {:2.4f} feet\".format(sd_nyc_total/np.mean(phis_nyc_total)))\n",
    "\n",
    "    # USGS\n",
    "    phis_usgs_total = [ss.phi_usgs_total for ss in SampleFlightList]\n",
    "    phis_usgs_sample = [ss.phi_usgs_sample for ss in SampleFlightList]\n",
    "    avg_flight_paths_usgs = np.mean([len(flight.flight_list_usgs) for flight in SampleFlightList])-2\n",
    "\n",
    "    sd_usgs_total = np.mean([ss.flight_list_usgs[0].sd_dist for ss in SampleFlightList])\n",
    "    sd_usgs_sample = np.mean([ss.flight_list_usgs[1].sd_dist for ss in SampleFlightList])\n",
    "\n",
    "    print(\"\\n2014 scan (Horizontal, {} samples): \\n\".format(len(SampleFlightList))+\"*\"*30)\n",
    "    print(\"Avg points per square: {:2.2f} points\".format((4 * feet_from_point**2)*np.mean(pt_density_usgs_list)))\n",
    "    print(\"Avg density: {:2.4f} pts/sqft (SD: {:2.4f})\".format(np.mean(pt_density_usgs_list),np.std(pt_density_usgs_list)))\n",
    "    print(\"Avg number of flight paths per square: {:2.2f}\".format(avg_flight_paths_usgs))\n",
    "    print(\"\\nphi_total: {:2.4f} (SD: {:2.4f})\".format(np.mean(phis_usgs_total),np.std(phis_usgs_total)))\n",
    "    print(\"phi_sample: {:2.4f} (SD: {:2.4f})\".format(np.mean(phis_usgs_sample),np.std(phis_usgs_sample)))\n",
    "    print(\"Total point dist from plane, SD: {:2.4f} feet\".format(sd_usgs_total))\n",
    "    print(\"Avg flight point dist from plane, SD: {:2.4f} feet\".format(sd_usgs_total/np.mean(phis_usgs_total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling squares and Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Horizontal Density\n",
    "The section below iterates through the sample surfaces, generating sample squares, generating density/accuracy statistics for each, and gathering the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregators - collect sample squares across multiple sample areas\n",
    "pt_density_usgs_list, pt_density_nyc_list, pt_density_laefer_list = [],[],[]\n",
    "avg_height_diff = []\n",
    "sd_height_usgs, sd_height_nyc, sd_height_laefer = [],[],[]\n",
    "SampleFlightList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Water front parking lot\n",
    "pt1 = np.array([976534.92, 173979.05,0])\n",
    "pt2 = np.array([976863.17, 174360.18,0])\n",
    "u_length = np.linalg.norm(pt2-pt1)\n",
    "v_length = 80\n",
    "print(\"u_length: \",u_length)\n",
    "pt3 = np.array([976595.53720051, 173926.84315177,0]) # Backed into from previous 2D v calculation\n",
    "uv_inv,w,unit_u,unit_v = rectangle(pt1[:2],pt2[:2],500,80)\n",
    "\n",
    "rectangle_points_laefer = pd.read_pickle(file_dir+\"rectangle_points_laefer.pkl\")\n",
    "rectangle_points_nyc = pd.read_pickle(file_dir+\"rectangle_points_nyc.pkl\")\n",
    "rectangle_points_usgs = pd.read_pickle(usgs_file_dir+\"rectangle_points_usgs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create center_points\n",
    "feet_from_point = 3.281/np.sqrt(2)\n",
    "center_points = center_point_sample(5000,pt1,pt2,pt3,u_length=u_length,v_length=v_length,border=[0.05,0.05])\n",
    "mean_z = rectangle_points_laefer['z_scaled'].mean()\n",
    "pts_thrown_out = 0\n",
    "\n",
    "for center_point in center_points:\n",
    "    square_points_usgs = in_horizontal_square(rectangle_points_usgs,center_point[:2],feet_from_point)\n",
    "    square_points_nyc = in_horizontal_square(rectangle_points_nyc,center_point[:2],feet_from_point)\n",
    "    square_points_laefer = in_horizontal_square(rectangle_points_laefer,center_point[:2],feet_from_point)\n",
    "    \n",
    "    laefer_flight_count = len(square_points_laefer['flight_id'].unique())\n",
    "    nyc_flight_count = len(square_points_nyc['flight_id'].unique())\n",
    "    usgs_flight_count = len(square_points_usgs['flight_id'].unique())\n",
    "   \n",
    "    laefer_density = square_points_laefer.shape[0] / laefer_flight_count\n",
    "    try:\n",
    "        nyc_density = square_points_nyc.shape[0]/nyc_flight_count\n",
    "    except ZeroDivisionError:\n",
    "        nyc_density = 0\n",
    "        \n",
    "    try:\n",
    "        usgs_density = square_points_usgs.shape[0] / usgs_flight_count\n",
    "    except ZeroDivisionError:\n",
    "        usgs_density = 0\n",
    " \n",
    "    # If z_max > 10, <6 for any dataset, skip it\n",
    "    if (square_points_nyc['z_scaled'].max()<mean_z+3) & \\\n",
    "       (square_points_usgs['z_scaled'].max()<mean_z+3) & \\\n",
    "       (square_points_laefer['z_scaled'].max()<mean_z+3):\n",
    "        # Statistics\n",
    "                \n",
    "        # Point density\n",
    "        num_points_nyc = square_points_nyc.shape[0]\n",
    "        pt_density_nyc_list.append(num_points_nyc / (4 * feet_from_point**2))        \n",
    "        num_points_usgs = square_points_usgs.shape[0]\n",
    "        pt_density_usgs_list.append(num_points_usgs / (4 * feet_from_point**2))        \n",
    "        num_points_laefer = square_points_laefer.shape[0]\n",
    "        pt_density_laefer_list.append(num_points_laefer / (4 * feet_from_point**2))        \n",
    "        \n",
    "        # Flight path specifics\n",
    "        laefer_flight_list = create_flight_list(square_points_laefer)\n",
    "        usgs_flight_list = create_flight_list(square_points_usgs)\n",
    "        nyc_flight_list = create_flight_list(square_points_nyc)\n",
    "        # Create SampleSquare from all flight passes\n",
    "        ss = SampleSquare(laefer_flight_list, nyc_flight_list,usgs_flight_list, x = center_point[0], \\\n",
    "                          y=center_point[1],feet_from_point=feet_from_point)\n",
    "        # Collect SampleSquares\n",
    "        SampleFlightList.append(ss)\n",
    "        \n",
    "    else:\n",
    "        pts_thrown_out +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_out(SampleFlightList,pt_density_laefer_list,pt_density_nyc_list,pt_density_usgs_list,feet_from_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back parking lot\n",
    "u_length = 200\n",
    "v_length = 40\n",
    "pt1 = np.array([977221.16,173403.09,0])\n",
    "pt2 = np.array([977345.97611538, 173559.36199794,0])\n",
    "uv_inv,w,unit_u,unit_v = rectangle(pt1[:2],pt2[:2],u_length,v_length)\n",
    "\n",
    "pt3 = np.concatenate((w + unit_v*v_length,np.zeros(1)))\n",
    "rectangle_points_laefer=pd.read_pickle(file_dir+\"rectangle_points_back_parking_laefer.pkl\")\n",
    "rectangle_points_nyc=pd.read_pickle(file_dir+\"rectangle_points_back_parking_nyc.pkl\")\n",
    "rectangle_points_usgs=pd.read_pickle(usgs_file_dir+\"rectangle_points_back_parking_usgs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create center_points\n",
    "feet_from_point = 3.281/np.sqrt(2)\n",
    "center_points = center_point_sample(5000,pt1,pt2,pt3,u_length=u_length,v_length=v_length,border=[0.05,0.05])\n",
    "mean_z = rectangle_points_laefer['z_scaled'].mean()\n",
    "pts_thrown_out = 0\n",
    "\n",
    "for center_point in center_points:\n",
    "    square_points_usgs = in_horizontal_square(rectangle_points_usgs,center_point[:2],feet_from_point)\n",
    "    square_points_nyc = in_horizontal_square(rectangle_points_nyc,center_point[:2],feet_from_point)\n",
    "    square_points_laefer = in_horizontal_square(rectangle_points_laefer,center_point[:2],feet_from_point)\n",
    "    \n",
    "    laefer_flight_count = len(square_points_laefer['flight_id'].unique())\n",
    "    nyc_flight_count = len(square_points_nyc['flight_id'].unique())\n",
    "    usgs_flight_count = len(square_points_usgs['flight_id'].unique())\n",
    "   \n",
    "    laefer_density = square_points_laefer.shape[0] / laefer_flight_count\n",
    "    try:\n",
    "        nyc_density = square_points_nyc.shape[0]/nyc_flight_count\n",
    "    except ZeroDivisionError:\n",
    "        nyc_density = 0\n",
    "    \n",
    "    try:\n",
    "        usgs_density = square_points_usgs.shape[0] / usgs_flight_count\n",
    "    except ZeroDivisionError:\n",
    "        usgs_density = 0\n",
    " \n",
    "    # If z_max > 10, <6 for any dataset, skip it\n",
    "    if (square_points_nyc['z_scaled'].max()<mean_z+3) & \\\n",
    "       (square_points_usgs['z_scaled'].max()<mean_z+3) & \\\n",
    "       (square_points_laefer['z_scaled'].max()<mean_z+3):\n",
    "        # Statistics\n",
    "                \n",
    "        # Point density\n",
    "        num_points_nyc = square_points_nyc.shape[0]\n",
    "        pt_density_nyc_list.append(num_points_nyc / (4 * feet_from_point**2))        \n",
    "        num_points_usgs = square_points_usgs.shape[0]\n",
    "        pt_density_usgs_list.append(num_points_usgs / (4 * feet_from_point**2))        \n",
    "        num_points_laefer = square_points_laefer.shape[0]\n",
    "        pt_density_laefer_list.append(num_points_laefer / (4 * feet_from_point**2))        \n",
    "        \n",
    "        # Flight path specifics\n",
    "        laefer_flight_list = create_flight_list(square_points_laefer)\n",
    "        usgs_flight_list = create_flight_list(square_points_usgs)\n",
    "        nyc_flight_list = create_flight_list(square_points_nyc)\n",
    "        # Create SampleSquare from all flight passes\n",
    "        ss = SampleSquare(laefer_flight_list, nyc_flight_list,usgs_flight_list, x = center_point[0], \\\n",
    "                          y=center_point[1],feet_from_point=feet_from_point)\n",
    "        # Collect SampleSquares\n",
    "        SampleFlightList.append(ss)\n",
    "        \n",
    "    else:\n",
    "        pts_thrown_out +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_out(SampleFlightList,pt_density_laefer_list,pt_density_nyc_list,pt_density_usgs_list,feet_from_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw_df = pd.DataFrame([flight.error_decomp_laefer for flight in SampleFlightList],columns=['laefer_C','laefer_W','laefer_rmse'])\n",
    "cw_df['nyc_C'] = [flight.error_decomp_nyc[0] for flight in SampleFlightList]\n",
    "cw_df['nyc_W'] = [flight.error_decomp_nyc[1] for flight in SampleFlightList]\n",
    "cw_df['nyc_rmse'] = [flight.error_decomp_nyc[2] for flight in SampleFlightList]\n",
    "\n",
    "cw_df['usgs_C'] = [flight.error_decomp_usgs[0] for flight in SampleFlightList]\n",
    "cw_df['usgs_W'] = [flight.error_decomp_usgs[1] for flight in SampleFlightList]\n",
    "cw_df['usgs_rmse'] = [flight.error_decomp_usgs[2] for flight in SampleFlightList]\n",
    "\n",
    "(cw_df/3.28084).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all heights by flight id\n",
    "from collections import defaultdict\n",
    "\n",
    "flight_id_dict = defaultdict(list)\n",
    "\n",
    "for sample_num in range(len(SampleFlightList)):\n",
    "    try:\n",
    "        dd = dict([(fp.flight_id,fp.h) for fp in SampleFlightList[sample_num].flight_list_laefer[2:]])\n",
    "        for key in dd.keys():\n",
    "            flight_id_dict[key].append(dd[key])\n",
    "    except AttributeError: \n",
    "        pass\n",
    "\n",
    "h_dist = []\n",
    "for key in flight_id_dict.keys():\n",
    "    mean_h = np.mean([abs(v) for v in flight_id_dict[key]])\n",
    "    h_dist.append(1000*mean_h)\n",
    "    print(\"{:2}: {:2.4f}\".format(key,mean_h))\n",
    "\n",
    "# Plot distribution of mean abs(heights)\n",
    "plt.hist(h_dist)\n",
    "plt.title(\"Dublin Horizontal Surfaces\",fontsize=12)\n",
    "plt.xlabel(\"Mean absolute height (mm)\",fontsize=12)\n",
    "plt.ylabel(\"Distribution of flight passes\",fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_rmse = [ss.flight_list_laefer[0].square_dist \\\n",
    "/ss.flight_list_laefer[0].num_points for ss in SampleFlightList]\n",
    "np.sqrt(l_rmse).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw_df['laefer_rmse'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw_df['nyc_rmse'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Distance from plane\\n\"+\"*\"*30)\n",
    "print(\"2019: {:2.4f} (SD: {:2.4f})\".format(np.mean([ss.delta_h_mean_laefer for ss in SampleFlightList]),np.mean([ss.delta_h_sd_laefer for ss in SampleFlightList])))\n",
    "print(\"2017: {:2.4f} (SD: {:2.4f})\".format(np.mean([ss.delta_h_mean_nyc for ss in SampleFlightList]),np.mean([ss.delta_h_sd_nyc for ss in SampleFlightList])))\n",
    "print(\"2014: {:2.4f} (SD: {:2.4f})\".format(np.mean([ss.delta_h_mean_usgs for ss in SampleFlightList]),np.mean([ss.delta_h_sd_usgs for ss in SampleFlightList])))\n",
    "\n",
    "print(\"\\nCosine Similarity\\n\"+\"*\"*30)\n",
    "print(\"{:2.4f}\".format(np.mean([np.mean(ss.cosine_sim_matrix_laefer) for ss in SampleFlightList])))\n",
    "print(\"{:2.4f}\".format(np.mean([np.mean(ss.cosine_sim_matrix_nyc) for ss in SampleFlightList])))\n",
    "print(\"{:2.4f}\".format(np.mean([np.mean(ss.cosine_sim_matrix_usgs) for ss in SampleFlightList])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertical density\n",
    "The section below iterates through the sample surfaces, generating sample squares, generating density/accuracy statistics for each, and gathering the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vert_density(square_points_list, \n",
    "                 middle_pt, \n",
    "                 right_pt, \n",
    "                 left_pt, \n",
    "                 bottom_left_pt, \n",
    "                 feet_from_pt,\n",
    "                 SampleFlightList=[],\n",
    "                 pt_density_list=[[],[],[]],\n",
    "                 sd_wall_list=[[],[],[]],\n",
    "                 wall_face_list=None\n",
    "                 ):\n",
    "    # Unzip the list\n",
    "    square_points_laefer,square_points_nyc,square_points_usgs = \\\n",
    "    square_points_list[0],square_points_list[1],square_points_list[2]\n",
    "    \n",
    "    pt_density_laefer_list,pt_density_nyc_list,pt_density_usgs_list = \\\n",
    "    pt_density_list[0],pt_density_list[1],pt_density_list[2]\n",
    "    \n",
    "    sd_wall_dist_laefer,sd_wall_dist_nyc,sd_wall_dist_usgs = \\\n",
    "    sd_wall_list[0],sd_wall_list[1],sd_wall_list[2] \n",
    "    # Create aggregators if not provided\n",
    "    \n",
    "    \n",
    "    # Calculate norm_vector from 3 points, to define plane and extract the wall face\n",
    "    norm = np.cross(middle_pt - right_pt,(left_pt-right_pt))\n",
    "    norm = norm / np.linalg.norm(norm)\n",
    "\n",
    "    # Extract the wall face, above 15ft and below 120ft to avoid roof and grouund points\n",
    "    wall_face_laefer = grab_wall_face(square_points_laefer,norm, middle_pt,15,110,5e-1)\n",
    "    wall_face_nyc = grab_wall_face(square_points_nyc,norm, middle_pt,15,110,5e-1)\n",
    "    wall_face_usgs = grab_wall_face(square_points_usgs,norm, middle_pt,15,110,5e-1)\n",
    "\n",
    "    # Fit a plane, create norm_vector, calculate dist_from_plane\n",
    "    norm_vector_laefer,_,wall_face_laefer,_ = plane_fit(wall_face_laefer) \n",
    "    norm_vector_nyc,_,wall_face_nyc,_ = plane_fit(wall_face_nyc) \n",
    "    norm_vector_usgs,_,wall_face_usgs,_ = plane_fit(wall_face_usgs) \n",
    "    \n",
    "    # Calculate the rectangle side length, based on points\n",
    "    u_length = np.linalg.norm(bottom_left_pt-top_left_pt)\n",
    "    v_length = np.linalg.norm(bottom_left_pt-bottom_right_pt)\n",
    "    print(\"u_length: {:2.2f}m\".format(u_length/3.28084))\n",
    "    print(\"v_length: {:2.2f}m\".format(v_length/3.28084))\n",
    "\n",
    "    # Sample points in wall\n",
    "    #Note 0.28 border_v comes from: (3.5/2) (half-meter feet_from_point) / v_length (with some buffer)\n",
    "    feet_from_pt_v = 3.5\n",
    "    feet_from_pt_u = 3.5\n",
    "    border = [0,0]\n",
    "    border[0] = 1.08*(feet_from_pt_u / u_length)\n",
    "    border[1] = 1.08*(feet_from_pt_v / v_length)\n",
    "    # For density up the wall below\n",
    "    try:\n",
    "        center_points_old = center_points\n",
    "    except:\n",
    "        pass\n",
    "    center_points = center_point_sample(2500,\n",
    "                        bottom_left_pt,top_left_pt,bottom_right_pt,\n",
    "                        u_length=u_length,v_length=v_length,border=border)\n",
    "    \n",
    "    # Main Loop through the sample points\n",
    "    pts_thrown_out = 0\n",
    "\n",
    "    for center_point in center_points:\n",
    "        square_points_usgs,pt_density_usgs = in_vertical_square(wall_face_usgs,\n",
    "                                                    norm_vector_usgs,\n",
    "                                                    center_point,\n",
    "                                                    feet_from_pt_v,\n",
    "                                                    feet_from_pt_u)\n",
    "        square_points_nyc,pt_density_nyc = in_vertical_square(wall_face_nyc,\n",
    "                                                    norm_vector_nyc,\n",
    "                                                    center_point,\n",
    "                                                    feet_from_pt_v,\n",
    "                                                    feet_from_pt_u)\n",
    "        square_points_laefer,pt_density_laefer = in_vertical_square(wall_face_laefer,\n",
    "                                                    norm_vector_laefer,\n",
    "                                                    center_point,\n",
    "                                                    feet_from_pt_v,\n",
    "                                                    feet_from_pt_u)\n",
    "        laefer_flight_count = len(square_points_laefer['flight_id'].unique())\n",
    "        nyc_flight_count = len(square_points_nyc['flight_id'].unique())\n",
    "        usgs_flight_count = len(square_points_usgs['flight_id'].unique())\n",
    "\n",
    "        # Statistics\n",
    "\n",
    "        # Point density\n",
    "        num_points_laefer = square_points_laefer.shape[0]\n",
    "        pt_density_laefer_list.append(pt_density_laefer)        \n",
    "        num_points_usgs = square_points_usgs.shape[0]\n",
    "        pt_density_usgs_list.append(pt_density_usgs)        \n",
    "        num_points_nyc = square_points_nyc.shape[0]\n",
    "        pt_density_nyc_list.append(pt_density_nyc)        \n",
    "        \n",
    "\n",
    "        # Fit a plane\n",
    "        if square_points_laefer.shape[0] > 0:\n",
    "            norm_vector_laefer,_,square_points_laefer,_ = plane_fit(square_points_laefer)\n",
    "            laefer_flight_list = create_flight_list(square_points_laefer)\n",
    "        else:\n",
    "            laefer_flight_list=None\n",
    "\n",
    "        if square_points_nyc.shape[0] > 0:\n",
    "            norm_vector_nyc,_,square_points_nyc,_ = plane_fit(square_points_nyc)\n",
    "            nyc_flight_list = create_flight_list(square_points_nyc)\n",
    "        else:\n",
    "            nyc_flight_list=None\n",
    "\n",
    "        if square_points_usgs.shape[0] > 0:\n",
    "            norm_vector_usgs,_,square_points_usgs,_ = plane_fit(square_points_usgs)\n",
    "            usgs_flight_list = create_flight_list(square_points_usgs)\n",
    "        else:\n",
    "            usgs_flight_list=None\n",
    "        # Flight path specifics\n",
    "        ss = SampleSquare(laefer_flight_list, nyc_flight_list,usgs_flight_list, x = center_point[0], \\\n",
    "                          y=center_point[1],z=center_point[2],feet_from_point=[feet_from_pt_v,feet_from_pt_u])\n",
    "        SampleFlightList.append(ss)    \n",
    "    \n",
    "    # Print outs\n",
    "    print(\"Pts in Laefer: {}\".format(wall_face_laefer.shape[0]))\n",
    "    print(\"Pts in NYC: {}\".format(wall_face_nyc.shape[0]))\n",
    "    print(\"Pts in USGS: {}\".format(wall_face_usgs.shape[0]))\n",
    "\n",
    "    print(\"Vertical face point density over {:d} samples\".format(len(pt_density_laefer_list)))\n",
    "    print(\"*\"*30)\n",
    "    print(\"USGS avg density: {:2.4f} (SD: {:2.4f})\".format(np.mean(pt_density_usgs_list),np.std(pt_density_usgs_list)))\n",
    "    print(\"NYC avg density: {:2.4f} (SD: {:2.4f})\".format(np.mean(pt_density_nyc_list),np.std(pt_density_nyc_list)))\n",
    "    print(\"Laefer avg density: {:2.4f} (SD: {:2.4f})\".format(np.mean(pt_density_laefer_list),np.std(pt_density_laefer_list)))\n",
    "    \n",
    "    # Output\n",
    "    wall_face_list = [wall_face_laefer,wall_face_nyc,wall_face_usgs]\n",
    "    pt_density_list = [pt_density_laefer_list,pt_density_nyc_list,pt_density_usgs_list]\n",
    "    sd_wall_list = [sd_wall_dist_laefer,sd_wall_dist_nyc,sd_wall_dist_usgs]\n",
    "    \n",
    "    return SampleFlightList, wall_face_list, pt_density_list, sd_wall_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregators\n",
    "pt_density_list= [[],[],[]]\n",
    "sd_wall_list=[[],[],[]]\n",
    "SampleFlightList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other side of the windows (A)\n",
    "middle_pt = np.array([977214.86,174562.64,58.762])\n",
    "right_pt = np.array([977213.577,174561.01,111.614])\n",
    "left_pt = np.array([977217.294, 174565.64, 33.483])\n",
    "bottom_left_pt= np.array([977217.09,174565.64,25.809])\n",
    "top_left_pt = np.array([977217.09,174565.64,115.80])\n",
    "bottom_right_pt = np.array([977213.026,174560.064,25.809])\n",
    "feet_from_pt = 3.5\n",
    "\n",
    "# Right vertical wall\n",
    "square_points_vertical_laefer = pd.read_pickle(\"../../Data/parking_lot/right_wall_vertical_laefer.pkl\")\n",
    "square_points_vertical_nyc = pd.read_pickle(\"../../Data/parking_lot/right_wall_vertical_nyc.pkl\")\n",
    "square_points_vertical_usgs = pd.read_pickle(\"../../Data/parking_lot/right_wall_vertical_usgs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "square_points_list = [square_points_vertical_laefer,square_points_vertical_nyc,square_points_vertical_usgs]\n",
    "SampleFlightList, wall_face_list, pt_density_list, sd_wall_list = \\\n",
    "vert_density(square_points_list, \n",
    "                 middle_pt, \n",
    "                 right_pt, \n",
    "                 left_pt, \n",
    "                 bottom_left_pt, \n",
    "                 feet_from_pt,\n",
    "                 SampleFlightList,    \n",
    "                 pt_density_list,\n",
    "                 sd_wall_list\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wall_face_nyc = wall_face_list[2]\n",
    "flight_ids = wall_face_nyc['flight_id'].unique()\n",
    "wall_face_nyc['pts_bins'] = pd.cut((wall_face_nyc['z_scaled']-9)/3.28084, \\\n",
    "                                      bins=range(0,40,3), \\\n",
    "                                      labels=range(3,40,3))\n",
    "\n",
    "for f in flight_ids:\n",
    "    flight_pts = wall_face_nyc[wall_face_nyc['flight_id']==f]\n",
    "    # Num of points\n",
    "    print(\"\\n\\nDensity for flight id {}:\".format(f),flight_pts.groupby('pts_bins')['X'].count()/(3*2.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer wall (B)\n",
    "middle_pt = np.array([977223.71,174573.7,75.141])\n",
    "right_pt = np.array([977221.44, 174571.037, 68.44])\n",
    "left_pt = np.array([977226.47, 174577.23, 36.958])\n",
    "bottom_left_pt = np.array([977225.752, 174576.37, 25.393])\n",
    "top_left_pt = np.array([977225.752, 174576.37, 117.937])\n",
    "bottom_right_pt = np.array([977221.605,174571.074,25.393])\n",
    "feet_from_pt = 3.5\n",
    "\n",
    "# Left vertical wall\n",
    "square_points_vertical_laefer = pd.read_pickle(\"../../Data/parking_lot/left_wall_vertical_laefer.pkl\")\n",
    "square_points_vertical_nyc = pd.read_pickle(\"../../Data/parking_lot/left_wall_vertical_nyc.pkl\")\n",
    "square_points_vertical_usgs = pd.read_pickle(\"../../Data/parking_lot/left_wall_vertical_usgs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "square_points_list = [square_points_vertical_laefer,square_points_vertical_nyc,square_points_vertical_usgs]\n",
    "SampleFlightList, wall_face_list, pt_density_list, sd_wall_list = \\\n",
    "vert_density(square_points_list, \n",
    "                 middle_pt, \n",
    "                 right_pt, \n",
    "                 left_pt, \n",
    "                 bottom_left_pt, \n",
    "                 feet_from_pt,\n",
    "                 SampleFlightList,  \n",
    "                 pt_density_list,\n",
    "                 sd_wall_list\n",
    "                 )\n",
    "\n",
    "# # Append wall_face to wall_face_laefer_total\n",
    "# wall_face_laefer_total = wall_face_laefer_total.append(wall_face_list[0])\n",
    "# wall_face_laefer_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wall_face_nyc = wall_face_list[2]\n",
    "flight_ids = wall_face_nyc['flight_id'].unique()\n",
    "wall_face_nyc['pts_bins'] = pd.cut((wall_face_nyc['z_scaled']-9)/3.28084, \\\n",
    "                                      bins=range(0,40,3), \\\n",
    "                                      labels=range(3,40,3))\n",
    "\n",
    "for f in flight_ids:\n",
    "    flight_pts = wall_face_nyc[wall_face_nyc['flight_id']==f]\n",
    "    # Num of points\n",
    "    print(\"\\n\\nDensity for flight id {}:\".format(f),flight_pts.groupby('pts_bins')['X'].count()/(3*2.05))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wall at other end of Army Terminal Bldg (C)\n",
    "middle_pt = np.array([976665.066,173867.613,75.54])\n",
    "right_pt = np.array([976663.22,173865.13,28.945])\n",
    "# bottom_left_pt= np.array([976665.002991,173867.726,13.903])\n",
    "# top_left_pt = np.array([976665.002991,173867.726,110.903])\n",
    "left_pt = np.array([976666.636,173869.511,54.121])\n",
    "bottom_left_pt= np.array([976665.46,173868.117,13.903])\n",
    "top_left_pt = np.array([976665.46,173868.117,110.903])\n",
    "bottom_right_pt = np.array([976662.514,173864.906,15.875])\n",
    "\n",
    "feet_from_pt = 3.5\n",
    "\n",
    "square_points_vertical_laefer = pd.read_pickle(\"../../Data/parking_lot/wall_c_laefer.pkl\")\n",
    "square_points_vertical_nyc = pd.read_pickle(\"../../Data/parking_lot/wall_c_nyc.pkl\")\n",
    "square_points_vertical_usgs = pd.read_pickle(\"../../Data/parking_lot/wall_c_usgs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_points_vertical_laefer = square_points_vertical_laefer[square_points_vertical_laefer['x_scaled']<976667]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_points_list = [square_points_vertical_laefer,square_points_vertical_nyc,square_points_vertical_usgs]\n",
    "SampleFlightList, wall_face_list, pt_density_list, sd_wall_list = \\\n",
    "vert_density(square_points_list, \n",
    "                 middle_pt, \n",
    "                 right_pt, \n",
    "                 left_pt, \n",
    "                 bottom_left_pt, \n",
    "                 feet_from_pt,\n",
    "                 SampleFlightList,      \n",
    "                 pt_density_list,\n",
    "                 sd_wall_list\n",
    "                 )\n",
    "# # Append wall_face to wall_face_laefer_total\n",
    "# wall_face_laefer_total = wall_face_laefer_total.append(wall_face_list[0])\n",
    "# wall_face_laefer_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wall_face_nyc = wall_face_list[0]\n",
    "flight_ids = wall_face_nyc['flight_id'].unique()\n",
    "wall_face_nyc['pts_bins'] = pd.cut((wall_face_nyc['z_scaled']-9)/3.28084, \\\n",
    "                                      bins=range(0,40,3), \\\n",
    "                                      labels=range(3,40,3))\n",
    "\n",
    "for f in flight_ids:\n",
    "    flight_pts = wall_face_nyc[wall_face_nyc['flight_id']==f]\n",
    "    # Num of points\n",
    "    print(\"\\n\\nDensity for flight id {}:\".format(f),flight_pts.groupby('pts_bins')['X'].count()/(3*1.29))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Wall in middle of Army Terminal Bldg (D)\n",
    "middle_pt = np.array([976763.544,173992.004,23.858])\n",
    "right_pt = np.array([976762.356,173990.422,101.224])\n",
    "left_pt = np.array([976765.87,173994.878,87.094])\n",
    "bottom_left_pt= np.array([976765.87,173994.878,13.9])\n",
    "top_left_pt = np.array([976765.87,173994.878,110.9])\n",
    "bottom_right_pt = np.array([976762.356,173990.422,13.9])\n",
    "\n",
    "feet_from_pt = 3.5/2\n",
    "\n",
    "square_points_vertical_laefer = pd.read_pickle(\"../../Data/parking_lot/wall_d_laefer.pkl\")\n",
    "square_points_vertical_nyc = pd.read_pickle(\"../../Data/parking_lot/wall_d_nyc.pkl\")\n",
    "square_points_vertical_usgs = pd.read_pickle(\"../../Data/parking_lot/wall_d_usgs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_points_list = [square_points_vertical_laefer,square_points_vertical_nyc,square_points_vertical_usgs]\n",
    "SampleFlightList, wall_face_list, pt_density_list, sd_wall_list = \\\n",
    "vert_density(square_points_list, \n",
    "                 middle_pt, \n",
    "                 right_pt, \n",
    "                 left_pt, \n",
    "                 bottom_left_pt, \n",
    "                 feet_from_pt,\n",
    "                 SampleFlightList,  \n",
    "                 pt_density_list,\n",
    "                 sd_wall_list\n",
    "                 )\n",
    "# # Append wall_face to wall_face_laefer_total\n",
    "# wall_face_laefer_total = wall_face_laefer_total.append(wall_face_list[0])\n",
    "# wall_face_laefer_total.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wall_face_nyc = wall_face_list[0]\n",
    "flight_ids = wall_face_nyc['flight_id'].unique()\n",
    "wall_face_nyc['pts_bins'] = pd.cut((wall_face_nyc['z_scaled']-9)/3.28084, \\\n",
    "                                      bins=range(0,40,3), \\\n",
    "                                      labels=range(3,40,3))\n",
    "\n",
    "for f in flight_ids:\n",
    "    flight_pts = wall_face_nyc[wall_face_nyc['flight_id']==f]\n",
    "    # Num of points\n",
    "    print(\"\\n\\nDensity for flight id {}:\".format(f),flight_pts.groupby('pts_bins')['X'].count()/(3*1.73))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # South wall!  For vertical density of NYC\n",
    "# middle_pt = np.array([976617.27,173802.86,58])\n",
    "# right_pt = np.array([976620.67,173800.14,50])\n",
    "# left_pt = np.array([976615.24,173804.53,62])\n",
    "# bottom_left_pt = np.array([976615.24,173804.53,21.98])\n",
    "# top_left_pt = np.array([976615.24,173804.53,115])\n",
    "# bottom_right_pt = np.array([976620.67,173800.14,21.98])\n",
    "# feet_from_pt = 3.5/2\n",
    "\n",
    "# square_points_vertical_laefer = pd.read_pickle(\"../../Data/parking_lot/south_outer_wall_vertical_laefer.pkl\")\n",
    "# square_points_vertical_nyc = pd.read_pickle(\"../../Data/parking_lot/south_outer_wall_vertical_nyc.pkl\")\n",
    "# square_points_vertical_usgs = pd.read_pickle(\"../../Data/parking_lot/south_outer_wall_vertical_usgs.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# square_points_list = [square_points_vertical_laefer,square_points_vertical_nyc,square_points_vertical_usgs]\n",
    "# SampleFlightList, wall_face_list, pt_density_list, sd_wall_list = \\\n",
    "# vert_density(square_points_list, \n",
    "#                  middle_pt, \n",
    "#                  right_pt, \n",
    "#                  left_pt, \n",
    "#                  bottom_left_pt, \n",
    "#                  feet_from_pt,\n",
    "#                  SampleFlightList,  \n",
    "#                  pt_density_list,\n",
    "#                  sd_wall_list\n",
    "#                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wall density for NYC flight pass\n",
    "wall_face_nyc = wall_face_list[1]\n",
    "flight_ids = ['181004','180819']\n",
    "wall_face_nyc['pts_bins'] = pd.cut((wall_face_nyc['z_scaled']-9)/3.28084, \\\n",
    "                                      bins=range(0,40,3), \\\n",
    "                                      labels=range(3,40,3))\n",
    "\n",
    "for f in wall_face_nyc['flight_id'].unique():\n",
    "    flight_pts = wall_face_nyc[wall_face_nyc['flight_id']==f]\n",
    "    # Num of points\n",
    "    print(\"\\n\\nDensity for flight id {}:\".format(f),flight_pts.groupby('pts_bins')['X'].count()/(3*2.13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wall_face_nyc = wall_face_list[1]\n",
    "plt.plot(wall_face_list[0]['x_scaled'],wall_face_list[0]['z_scaled'],'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_flight_paths_laefer = []\n",
    "for flight in SampleFlightList:\n",
    "    try:\n",
    "        avg_flight_paths_laefer.append(len(flight.flight_list_laefer)-2)\n",
    "    except AttributeError:\n",
    "        avg_flight_paths_laefer.append(0)\n",
    "\n",
    "print(\"Avg number of flight paths per square: {:2.2f}\".format(np.mean(avg_flight_paths_laefer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cw = []\n",
    "for flight in SampleFlightList:\n",
    "    try:\n",
    "        cw.append(flight.error_decomp_laefer)\n",
    "    except AttributeError:\n",
    "        print(\"Missing\")\n",
    "#     cw_df = pd.DataFrame(cw,columns=['laefer_C','laefer_W','laefer_rmse'])\n",
    "    cw_df = pd.DataFrame(cw,columns=['laefer_C','laefer_W','laefer_rmse'])\n",
    "\n",
    "(cw_df/3.28084).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all heights by flight id\n",
    "from collections import defaultdict\n",
    "\n",
    "flight_id_dict = defaultdict(list)\n",
    "\n",
    "for sample_num in range(len(SampleFlightList)):\n",
    "    try:\n",
    "        dd = dict([(fp.flight_id,fp.h) for fp in SampleFlightList[sample_num].flight_list_laefer[2:]])\n",
    "        for key in dd.keys():\n",
    "            flight_id_dict[key].append(dd[key])\n",
    "    except AttributeError: \n",
    "        pass\n",
    "\n",
    "h_dist = []\n",
    "for key in flight_id_dict.keys():\n",
    "    mean_h = np.mean([abs(v) for v in flight_id_dict[key]])\n",
    "    h_dist.append(1000*mean_h)\n",
    "    print(\"{:2}: {:2.4f}\".format(key,mean_h))\n",
    "\n",
    "# Plot distribution of mean abs(heights)\n",
    "plt.hist(h_dist)\n",
    "plt.title(\"Dublin Horizontal Surfaces\",fontsize=12)\n",
    "plt.xlabel(\"Mean absolute height (mm)\",fontsize=12)\n",
    "plt.ylabel(\"Distribution of flight passes\",fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw_df2 = pd.read_pickle(\"../../accuracy_data/vertical_cw.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cw_df2).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laefer\n",
    "sd_laefer_total = np.mean([ss.flight_list_laefer[0].sd_dist for ss in SampleFlightList])\n",
    "sd_laefer_sample = np.mean([ss.flight_list_laefer[1].sd_dist for ss in SampleFlightList])\n",
    "avg_flight_paths_laefer = np.mean([len(flight.flight_list_laefer) for flight in SampleFlightList])-2\n",
    "\n",
    "phis_laefer_total = [ss.phi_laefer_total for ss in SampleFlightList]\n",
    "phis_laefer_sample = [ss.phi_laefer_sample for ss in SampleFlightList]\n",
    "\n",
    "print(\"2019 scan (Vertical, {:d} samples)\\n\".format(len(pt_density_laefer_list))+\"*\"*30)\n",
    "print(\"Avg points per square: {:2.2f} points\".format((3.5*14)*np.mean(pt_density_laefer_list)))\n",
    "print(\"Avg density: {:2.4f} pts/sqft (SD: {:2.4f})\".format(np.mean(pt_density_laefer_list),np.std(pt_density_laefer_list)))\n",
    "print(\"Avg number of flight paths per square: {:2.2f}\".format(avg_flight_paths_laefer))\n",
    "\n",
    "print(\"\\nphi_total: {:2.4f} (SD: {:2.4f})\".format(np.mean(phis_laefer_total),np.std(phis_laefer_total)))\n",
    "print(\"phi_sample: {:2.4f} (SD: {:2.4f})\".format(np.mean(phis_laefer_sample),np.std(phis_laefer_total)))\n",
    "print(\"\\nTotal point dist from plane, SD: {:2.4f} feet\".format(sd_laefer_total))\n",
    "print(\"Avg flight point dist from plane, SD: {:2.4f} feet\".format(sd_laefer_total/np.mean(phis_laefer_total)))\n",
    "\n",
    "# phis_nyc_total = [ss.phi_nyc_total for ss in SampleFlightList]\n",
    "# phis_nyc_sample = [ss.phi_nyc_sample for ss in SampleFlightList]\n",
    "\n",
    "# sd_nyc_total = np.mean([ss.flight_list_nyc[0].sd_dist for ss in SampleFlightList])\n",
    "# sd_nyc_sample = np.mean([ss.flight_list_nyc[1].sd_dist for ss in SampleFlightList])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing points by scan angle\n",
    "\n",
    "Count the missing scan points based on scan angle between consecutive points.  Goal is to 1) compare the % of missing points for horizontal vs vertical surfaces, and 2) compare the % of missing points at different wall heights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wall_face_list[0].to_pickle(\"../../Data/parking_lot/wall_points_laefer.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pull_first_scan_gap(wall_face_laefer):\n",
    "    # Separate return num, only keep the first returns, add scan_gap, sort\n",
    "    wall_face_laefer['num_returns'] = np.floor(wall_face_laefer['flag_byte']/16).astype(int)\n",
    "    wall_face_laefer['return_num'] = wall_face_laefer['flag_byte']%16\n",
    "    first_return_wall = wall_face_laefer[wall_face_laefer['return_num']==1]\n",
    "    first_return_wall.sort_values(by=['gps_time'],inplace=True)\n",
    "    first_return_wall.reset_index(inplace=True)\n",
    "    first_return_wall.loc[1:,'scan_gap'] = [first_return_wall.loc[i+1,'scan_angle'] - first_return_wall.loc[i,'scan_angle'] for i in range(first_return_wall.shape[0]-1)]\n",
    "    first_return_wall.loc[0,'scan_gap'] = 0\n",
    "    return first_return_wall\n",
    "\n",
    "# Wall\n",
    "wall_face_laefer = wall_face_list[0]\n",
    "first_return_wall = pull_first_scan_gap(wall_face_laefer)\n",
    "# Rectangle\n",
    "rectangle_face_laefer = pd.read_pickle(file_dir+\"rectangle_points_laefer.pkl\")\n",
    "first_return_rectangle = pull_first_scan_gap(rectangle_face_laefer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan gap of -4,-5,-6 is normal, -10,-11,-15 are a missed point\n",
    "w = pd.DataFrame(first_return_rectangle['scan_gap'].value_counts())\n",
    "w.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan gap of -4,-5,-6 is normal, -10,-11,-15 are a missed point\n",
    "w = pd.DataFrame(first_return_wall[first_return_wall['flight_id']=='180819']['scan_gap'].value_counts())\n",
    "w.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the wall height into bins, compare % missing points at different heights\n",
    "\n",
    "first_return_wall['pts_bins'] = pd.cut((first_return_wall['z_scaled']-9)/3.28084, \\\n",
    "                                       bins=range(0,40,3),labels=range(3,40,3))\n",
    "\n",
    "first_return_wall['missed_point'] = np.zeros(first_return_wall.shape[0])\n",
    "first_return_wall['good_point'] = np.zeros(first_return_wall.shape[0])\n",
    "for index, row in first_return_wall.iterrows():\n",
    "    if (row['scan_gap'] >-17) & (row['scan_gap']< -6):\n",
    "        first_return_wall.loc[index,'missed_point']=1\n",
    "    if (row['scan_gap'] <-1) & (row['scan_gap'] > -7):\n",
    "        first_return_wall.loc[index,'good_point']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_return_wall[first_return_wall['z_scaled']<18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = first_return_wall[first_return_wall['flight_id']=='181004'].groupby('pts_bins').mean()\n",
    "a = first_return_wall.groupby('pts_bins').mean()\n",
    "\n",
    "a['miss_pct'] = a['missed_point']/a['good_point']\n",
    "a['miss_pct'][:-2].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a['miss_pct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(list(a['miss_pct'][:-2]),list(a['miss_pct'][:-2].index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intensity up and down the wall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wall_face_laefer = wall_face_list[0]\n",
    "flight_ids = ['181004','180819']\n",
    "wall_face_laefer['pts_bins'] = pd.cut((wall_face_laefer['z_scaled']-9)/3.28084,bins=range(0,40,3),labels=range(3,40,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wall_face_laefer['intensity'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_pts = wall_face_laefer[wall_face_laefer['flight_id']==flight_ids[1]]\n",
    "print(\"shape\",flight_pts.shape)\n",
    "flight_pts.groupby('pts_bins')['intensity'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intensity on the parking lot horizontal surface\n",
    "rectangle_points_laefer = pd.read_pickle(file_dir+\"rectangle_points_laefer.pkl\")\n",
    "rectangle_points_laefer[rectangle_points_laefer['flight_id']=='181004']['intensity'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density up and down the wall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts.loc['wall_horiz'] = np.sqrt((pts['x_scaled']-bottom_left_pt[0])**2+(pts['y_scaled']-bottom_left_pt[1])**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculates parallel pass density for NYC:\n",
    "# Overall density per flight pass: 1.02\n",
    "# Fraction of points in parallel flight pass: 16/18\n",
    "# Fraction of flight passes that are parallel: 1/2\n",
    "1.02*2*16/18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# horizontal distance along wall face\n",
    "# wall_horiz = np.sqrt((pts['x_scaled']-bottom_left_pt[0])**2+(pts['y_scaled']-bottom_left_pt[1])**2)\n",
    "\n",
    "wall_face_laefer = wall_face_list[1]\n",
    "plt.figure(figsize=[20,22])\n",
    "for i,fid in enumerate(wall_face_laefer['flight_id'].unique()):\n",
    "    plt.subplot(4,4,i+1)\n",
    "    pts = wall_face_laefer[wall_face_laefer['flight_id']==fid]\n",
    "    # Calculate horizontal wall location\n",
    "    pts['wall_horiz'] = np.sqrt((pts['x_scaled']-bottom_left_pt[0])**2+(pts['y_scaled']-bottom_left_pt[1])**2)\n",
    "    hist = pd.cut(wall_face_laefer[wall_face_laefer['flight_id']==fid]['z_scaled'],bins=range(0,120,5),labels=range(5,120,5))\n",
    "#     plt.hist(hist,orientation='horizontal')\n",
    "    plt.plot(pts['wall_horiz']/3.28084,pts['z_scaled']/3.28084,'x')\n",
    "    plt.yticks(np.arange(0,36,3))\n",
    "    plt.ylabel(\"Wall height (m)\")\n",
    "    plt.xlabel(\"Horizontal wall position (m)\")\n",
    "    plt.title(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# horizontal distance along wall face\n",
    "# wall_horiz = np.sqrt((pts['x_scaled']-bottom_left_pt[0])**2+(pts['y_scaled']-bottom_left_pt[1])**2)\n",
    "\n",
    "wall_face_laefer = wall_face_list[0]\n",
    "plt.figure(figsize=[20,22])\n",
    "for i,fid in enumerate(wall_face_laefer['flight_id'].unique()):\n",
    "    plt.subplot(4,4,i+1)\n",
    "    pts = wall_face_laefer[wall_face_laefer['flight_id']==fid]\n",
    "    # Calculate horizontal wall location\n",
    "    pts['wall_horiz'] = np.sqrt((pts['x_scaled']-bottom_left_pt[0])**2+(pts['y_scaled']-bottom_left_pt[1])**2)\n",
    "    hist = pd.cut(wall_face_laefer[wall_face_laefer['flight_id']==fid]['z_scaled'],bins=range(0,120,5),labels=range(5,120,5))\n",
    "#     plt.hist(hist,orientation='horizontal')\n",
    "    plt.plot(pts['wall_horiz']/3.28084,pts['z_scaled']/3.28084,'x')\n",
    "    plt.yticks(np.arange(0,36,3))\n",
    "    plt.ylabel(\"Wall height (m)\")\n",
    "    plt.xlabel(\"Horizontal wall position (m)\")\n",
    "    plt.title(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wall_face_laefer[(wall_face_laefer['flight_id']=='200742')& \\\n",
    "                 (wall_face_laefer['x_scaled']<977216.622) &\\\n",
    "                 (wall_face_laefer['x_scaled']>977216.62)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_ids = ['181004','180819']\n",
    "pts_fid = wall_face_laefer[wall_face_laefer['flight_id']==flight_ids[1]]['z_scaled'].copy()\n",
    "pts_fid = pts_fid/3.28084\n",
    "pts_bins = pd.cut(pts_fid,bins=range(15,36,3),labels=range(18,36,3))\n",
    "pts_density = pts_bins.value_counts()/(3*(6.899845795386761/3.28084))\n",
    "pts_density.sort_index(inplace=True)\n",
    "plt.plot(pts_density,range(18,36,3),'-o')\n",
    "plt.xlabel(\"Vertical Density (pts/m^2)\",fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.ylabel(\"Height on wall (m)\",fontsize=15)\n",
    "plt.yticks(fontsize=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts_fid = wall_face_laefer['z_scaled'].copy()\n",
    "pts_fid = pts_fid/3.28084\n",
    "pts_bins = pd.cut(pts_fid,bins=[0,16,32],labels=('low','high'))\n",
    "pts_density = pts_bins.value_counts()/(1*(6.899845795386761/3.28084))\n",
    "pts_density.sort_index(inplace=True)\n",
    "plt.bar(['low','high'],pts_density)\n",
    "plt.xlabel(\"Vertical Density (pts/m^2)\",fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.ylabel(\"Height on wall (m)\",fontsize=15)\n",
    "plt.yticks(fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts_fid = wall_face_laefer['z_scaled'].copy()\n",
    "pts_fid = pts_fid/3.28084\n",
    "pts_bins = pd.cut(pts_fid,bins=range(9,36,3),labels=range(12,36,3))\n",
    "pts_density = pts_bins.value_counts()/(2*(14.899845795386761/3.28084))\n",
    "pts_density.sort_index(inplace=True)\n",
    "plt.plot(pts_density,range(12,36,3),'-o')\n",
    "plt.xlabel(\"Vertical Density (pts/m^2)\",fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.ylabel(\"Height on wall (m)\",fontsize=15)\n",
    "plt.yticks(fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy up the wall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pts_fid = [sf.z for sf in SampleFlightList]\n",
    "pts_fid = [pt/3.28084 for pt in pts_fid]\n",
    "pts_bins = pd.cut(pts_fid,bins=range(8,36,2),labels=range(10,36,2))\n",
    "\n",
    "# Create acc_df Dataframe of accuracy and height bins for a specific flight id\n",
    "# acc_df = pd.DataFrame([ss.flight_list_laefer[0].sd_dist for ss in SampleFlightList],columns=['total_rmse'])\n",
    "acc_df = pd.DataFrame(pts_bins,columns=['bin'])\n",
    "\n",
    "acc_df['z'] = pts_fid\n",
    "# plt.plot(acc_df[acc_df['bin']==30]['z'])\n",
    "\n",
    "# Flight id specific accuracy\n",
    "fid = '181004' # Farther away \n",
    "# fid = '180819' # About 100m away\n",
    "acc_list = []\n",
    "for j in SampleFlightList:\n",
    "    dd = {j.flight_list_laefer[i].flight_id:i for i in range(len(j.flight_list_laefer))}\n",
    "    ix = dd[fid]\n",
    "    acc_list.append(j.flight_list_laefer[ix].sd_dist/3.28084)\n",
    "len(acc_list)\n",
    "\n",
    "acc_df['fid_rmse'] = acc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(np.arange(12-3,33-3,2),acc_df.groupby('bin')['fid_rmse'].mean().iloc[1:-1])\n",
    "plt.ylabel(\"Wall Height (m)\")\n",
    "plt.xlabel(\"Single Flight RMSE (m)\")\n",
    "plt.title(\"Flight ID: \"+str(fid))\n",
    "print(acc_df['bin'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(np.arange(12-3,33-3,2),acc_df.groupby('bin')['fid_rmse'].mean().iloc[1:-1])\n",
    "plt.ylabel(\"Wall Height (m)\")\n",
    "plt.xlabel(\"Single Flight RMSE (m)\")\n",
    "plt.title(\"Flight ID: \"+str(fid))\n",
    "print(acc_df['bin'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What causes C and W?\n",
    "What's the distribution of h's that generate C for vertical surfaces?  Outlier, or consistent misalignment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction = {\n",
    " '164239':'n-s  ',\n",
    " '164445':'n-s  ',\n",
    " '164640':'n-s  ',\n",
    " '172753':'e-w  ',\n",
    " '172928':'e-w  ',\n",
    " '173110':'e-w  ',\n",
    " '180632':'sw-ne',\n",
    " '180819':'sw-ne',\n",
    " '181004':'sw-ne',\n",
    " '200212':'se-nw',\n",
    " '200600':'se-nw',\n",
    " '200742':'se-nw',\n",
    " '200938':'se-nw' \n",
    "}\n",
    "dir_list = ['n-s  ','e-w  ','sw-ne','se-nw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# square_points_list, SampleFlightList, wall_face_list, pt_density_list, sd_wall_list\n",
    "h_dict = {\n",
    " '164239':[],\n",
    " '164445':[],\n",
    " '164640':[],\n",
    " '172753':[],\n",
    " '172928':[],\n",
    " '173110':[],\n",
    " '180632':[],\n",
    " '180819':[],\n",
    " '181004':[],\n",
    " '200212':[],\n",
    " '200600':[],\n",
    " '200742':[],\n",
    " '200938':[] \n",
    "}\n",
    "# Collect h for each flight pass in each sample square\n",
    "for sample_num in range(2500):\n",
    "    h_list = [fp.h for fp in SampleFlightList[sample_num].flight_list_laefer[2:]]\n",
    "    num_pt_list = [fp.num_points for fp in SampleFlightList[sample_num].flight_list_laefer[2:]]\n",
    "    flight_ids = [fp.flight_id for fp in SampleFlightList[sample_num].flight_list_laefer[2:]]\n",
    "    total_pts = SampleFlightList[sample_num].flight_list_laefer[0].num_points\n",
    "    for a in h_dict.keys():\n",
    "        if a in flight_ids:\n",
    "            ix = flight_ids.index(a)\n",
    "            h_dict[a].append(h_list[ix])\n",
    "        else:\n",
    "            h_dict[a].append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean height for each flight pass\n",
    "print(\"F_Id\\t Mean abs height\\n\",\"*\"*23)\n",
    "[print(\"{}: {:6.3f}\".format(k,np.nanmean([abs(h) for h in h_dict[k]]))) for k in h_dict.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(\"{}:\\t{:2.4f}\\t{:2} points\\t{:2.4f}\".format(flight_ids[i],h_list[i]**2*num_pt_list[i],num_pt_list[i],h_list[i])) for i in range(len(h_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "h_dir_dict = defaultdict(list)\n",
    "for ss in range(len(h_dict['164239'])):\n",
    "    for di in dir_list:\n",
    "        direction_h_sum = 0\n",
    "        direction_count = 0\n",
    "\n",
    "        for fp in h_dict.keys():\n",
    "            if direction[fp] == di:\n",
    "                direction_h_sum += h_dict[fp][ss]\n",
    "                direction_count +=1    \n",
    "        h_dir_dict[di].append(direction_h_sum/direction_count)\n",
    "dir_list = ['n-s  ','e-w  ','sw-ne','se-nw']\n",
    "# Mean height for each direction\n",
    "print(\"Direct\\tMean abs height\\n\",\"*\"*22)\n",
    "[print(\"{}: {:6.3f}\".format(k,np.nanmean([abs(h) for h in h_dir_dict[k]]))) for k in h_dir_dict.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix of mean height differences across flight passes (would expect larger diffs for different directions)\n",
    "def create_h_matrix(h_dict):\n",
    "    h_matrix = np.zeros([len(h_dict),len(h_dict)])\n",
    "    for i,key1 in enumerate(h_dict.keys()):\n",
    "        for j,key2 in enumerate(h_dict.keys()):\n",
    "            h_matrix[i,j] = np.nanmean(abs(np.array(h_dict[key1]) - np.array(h_dict[key2])))\n",
    "    return h_matrix\n",
    "\n",
    "def h_heatmap(h_matrix,h_dict,fontsize=15,label='Flight ID'):\n",
    "    # Heatmap of the matrix\n",
    "    plt.figure(figsize=[10,10])\n",
    "    plt.imshow(h_matrix, cmap='YlOrRd',vmin=0,vmax=0.6)\n",
    "    plt.xticks(np.arange(0,h_matrix.shape[0]),h_dict.keys(),rotation=90,fontsize=fontsize)\n",
    "    plt.yticks(np.arange(0,h_matrix.shape[0]),h_dict.keys(),fontsize=fontsize)\n",
    "    plt.ylabel(label,fontsize=fontsize)\n",
    "    plt.xlabel(label,fontsize=fontsize)\n",
    "    plt.ylim(h_matrix.shape[0]-0.5,-0.5)\n",
    "    plt.title(\"Mean Absolute h difference\",fontsize=fontsize)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "h_matrix = create_h_matrix(h_dict)\n",
    "h_heatmap(h_matrix,h_dict)  \n",
    "# h_matrix = create_h_matrix(h_dir_dict)\n",
    "# h_heatmap(h_matrix,h_dir_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "h_matrix = create_h_matrix(h_dir_dict)\n",
    "h_heatmap(h_matrix,h_dir_dict,label=\"Flight Direction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we remove the acute angle flight passes, what happens to C,W?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracted from SampleSquare class to remove specific flight passes\n",
    "def error_decomp_f(flight_list,skip_flight_pass_list=[]):\n",
    "    # Calculates the cross-pass, within-pass, and RMSE error for a particular SampleSquare and flight_list\n",
    "    C2,W2 = 0,0\n",
    "    \n",
    "    # num_points in skipped flight passes\n",
    "    num_pts_skip = 0\n",
    "    for flight in flight_list:\n",
    "        if flight.flight_id in skip_flight_pass_list:\n",
    "            num_pts_skip += flight.num_points\n",
    "    \n",
    "    for i,flight_i in enumerate(flight_list[2:]): # Skip total and sampled\n",
    "        if flight_i.flight_id not in skip_flight_pass_list:\n",
    "            C2 += flight_i.num_points*(flight_i.h**2) / (flight_list[0].num_points-num_pts_skip)\n",
    "            W2 += flight_i.num_points*(flight_i.sd_dist**2) / (flight_list[0].num_points - num_pts_skip)\n",
    "    rmse = np.sqrt(C2+W2)\n",
    "    C = np.sqrt(C2)\n",
    "    W = np.sqrt(W2)\n",
    "    return (C,W,rmse)\n",
    "\n",
    "# Worst 2 flight passes\n",
    "# skip_flight_pass_list = ['200600','200742']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original\n",
    "cw = []\n",
    "for flight in SampleFlightList:\n",
    "    try:\n",
    "        cw.append(flight.error_decomp_laefer)\n",
    "    except AttributeError:\n",
    "        print(\"Missing\")\n",
    "#     cw_df = pd.DataFrame(cw,columns=['laefer_C','laefer_W','laefer_rmse'])\n",
    "    cw_df = pd.DataFrame(cw,columns=['laefer_C','laefer_W','laefer_rmse'])\n",
    "\n",
    "(cw_df/3.28084).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only the sw-ne and se-nw flights\n",
    "skip_flight_pass_list = ['164239','164445','164640','172753','172928','173110']\n",
    "cw_aug = []\n",
    "for flight in SampleFlightList:\n",
    "    try:\n",
    "        cw_aug.append(error_decomp_f(flight.flight_list_laefer,skip_flight_pass_list))\n",
    "    except AttributeError:\n",
    "        print(\"Missing\")\n",
    "    cw_aug_df = pd.DataFrame(cw_aug,columns=['laefer_C','laefer_W','laefer_rmse'])\n",
    "\n",
    "(cw_aug_df/3.28084).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only the n-e and s-w flights\n",
    "skip_flight_pass_list = ['180632','180819','181004','200212','200600','200742','200938'] \n",
    "cw_aug = []\n",
    "for flight in SampleFlightList:\n",
    "    try:\n",
    "        cw_aug.append(error_decomp_f(flight.flight_list_laefer,skip_flight_pass_list))\n",
    "    except AttributeError:\n",
    "        print(\"Missing\")\n",
    "    cw_aug_df = pd.DataFrame(cw_aug,columns=['laefer_C','laefer_W','laefer_rmse'])\n",
    "\n",
    "(cw_aug_df/3.28084).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All except 2 worst flights\n",
    "skip_flight_pass_list =  ['200600','200742']\n",
    "cw_aug = []\n",
    "for flight in SampleFlightList:\n",
    "    try:\n",
    "        cw_aug.append(error_decomp_f(flight.flight_list_laefer,skip_flight_pass_list))\n",
    "    except AttributeError:\n",
    "        print(\"Missing\")\n",
    "    cw_aug_df = pd.DataFrame(cw_aug,columns=['laefer_C','laefer_W','laefer_rmse'])\n",
    "\n",
    "(cw_aug_df/3.28084).describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
